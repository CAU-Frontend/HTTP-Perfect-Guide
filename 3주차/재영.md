# 🕖 HTTP 완벽 가이드 7장(캐시) 🕖

## 웹 캐시💴💵💷

웹 캐시는 자주 쓰이는 리소스 사본을 자동으로 보관하는 HTTP 장치

요청이 도착했을 때, 캐시된 사본이 존재하면 서버가 아니라 캐시로부터 제공

캐시의 장점

1. 불필요한 데이터 전송 감소, 네트워크 요금 비용 감소
2. 네트워크 병목 감소, 대역폭 안늘려도 빠름
3. 서버에 대한 요청 감소, 서버 부하 감소
4. 거리로 인한 지연 감소

### 불필요한 데이터 전송

여러 클라이언트와 연결된 서버는 같은 리소스에 대한 요청이면 똑같은 바이트가 네트워크를 통해 반복해서 이동

네트워크 대역폭 사용, 전송 속도 저하, 서버 부하

캐시를 이용하면 첫 응답은 캐시에 보관, 이후 요청에는 캐시에 보관된 사본을 사용

### 대역폭 병목

네트워크는 보통 원격 서버보다 로컬 네트워크 클라이언트에 더 넓은 대역폭 제공

클라이언트가 서버에 접근할 때의 속도는 그 경로에 가장 느린 네트워크의 속도

빠른 LAN에 있는 캐시로부터 리소스를 받으면 성능을 대폭 개선 가능

### 갑작스러운 요청 쇄도

많은 사용자가 거의 동시에 리소스에 접근할 때, 불필요한 트래픽 급증과 네트워크 및 서버 장애 유발

### 거리로 인한 지연

먼 곳에 위치한 서버로 요청을 보내면 응답 지연

근처에 캐시를 설치한다면 거리 감소로 인한 속도 증가

### 적중과 부적중

요청이 왔을 때, 요청을 받은 리소스에 대한 사본이 있다면 요청 처리 가능 => 캐시 적중(cache hit)
없다면 서버로 요청 전달 => 캐시 부적중(cache miss)

### 재검사

서버의 리소스가 변경될 수 있음

캐시는 보관된 사본이 최신 리소스인지 점검 필요 => 신선도 검사 === HTTP 재검사

HTTP는 리소스 자체를 가져오지 않고도 최신인지 빠르게 검사할 수 있는 특별하고 효율적인 요청 정의

규칙 - 문서가 많을 때 대역폭은 부족하기 때문에 충분히 저장된지 오래된 경우에만 재검사 실시

재검사를 위해 사용되는 헤더 => If-Modifeied-Since

- 캐시된 시간 이후에 변경된 경우에만 사본을 보내달라는 의미

재검사 요청을 보냈을 때

- 리소스가 최신이면 - 304 Not Modified 응답 === 재검사 적중
- 리소스가 변경됐으면 - 200 OK + 새 리소스 응답 === 재검사 부적중
- 리소스가 제거됐으면 - 404 Not Found + 캐시는 사본 삭제 === 객체 삭제

### 캐시 토폴로지

개인 캐시

- 브라우저에서 사용하는 작은 캐시
- 자주 쓰이는 문서를 캐싱
- 브라우저를 통해 들여다보는 것도 가능

공용 캐시

- 프록시 캐시
- 캐시에서 리소스를 제공하거나 클라이언트입장에서 서버 접근
- 여러 클라이언트가 접근하기 때문에 불필요 트래픽 감소

### 캐시망, 콘텐츠 라우팅, 피어링

캐시망의 프록시 캐시는 복잡한 알고리즘으로 통신하여 어떤 캐시와 대화할건지, 서버로 우회할건지에 대한 결정을 동적으로 내림

캐시망 내부에 콘텐츠 라우팅을 위해 설계된 캐시는 다음과 같은 일을 할 수 있음

- URL에 근거하여, 부모 캐시 || 서버 중 하나를 동적으로 선택
- URL에 근거하여, 특정 부모 캐시 동적 선택
- 부모 캐시에게 가기 전에 캐시된 사본을 로컬에서 탐색
- 다른 캐시가 캐시된 리소스에 부분적 접근이 가능하도록 허용 but 캐시를 통한 트래픽이 다른 네트워크로 넘어가는 것은 허용 X

## 캐시 처리 단계

HTTP GET 메시지 하나를 처리하는 기본적인 캐시 처리 절차는 7단계로 이루어짐

### 1. 요청 받기

캐시는 네트워크로부터 도착한 요청 메시지 읽음

고성능 캐시는 여러 커넥션으로부터 데이터를 동시에 읽고 메시지 전체가 도착하기 전에 트랜잭션 처리

### 2. 파싱

캐시는 메시지를 파싱하여 URL과 헤더 추출

캐싱 소프트웨어가 헤더 필드를 처리하고 조작하기 편하도록 만듦

### 3. 검색

캐시를 로컬 복사본이 있는지 검사하고, 없으면 가져옴

캐시된 객체는 서버 본문과 서버 응답 헤더를 포함
객체가 얼마나 캐시에 머물렀는지에 대한 기록이나 사용 빈도수에 대한 메타데이터도 포함

### 4. 신선도 검사

HTTP 캐시는 일정 기간 동안 서버 문서의 사본을 보유하도록 함

이 기간을 넘겼다면 신선하지 않은 것으로 간주, 리소스 제공 전에 서버와 재검사 실시

### 5. 응답 생성

캐시된 서버 응답 헤더를 토대로 응답 헤더 생성

서버가 응답한 HTTP 버전이 클라이언트가 보낸 버전과 다륻면 캐시는 헤더를 적절하게 번역

신선도 정보를 삽입하며 요청이 프록시 캐시를 거쳐갔음을 알려주기 위해 Via 헤더 포함

캐시 Date 헤더는 캐시가 조정 x, 해당 리소스가 서버에서 최초로 생성된 일시를 표현한 것

### 6. 전송

커넥션 유지

응답이 준비되면 클라이언트에게 전송

### 7. 로깅

캐시는 로그 파일과 캐시 사용에 대한 통계 유지

캐시 트랜잭션 완료 후 통계 캐시 적중과 부적중 횟수 통계 갱신

로그 파일에는 요청 종류, url, 어떤 일이 일어났는지 알려주는 항목 추가

## 캐싱된 파일 신선도 유지

HTTP는 캐시된 리소스가 서버와 충분히 일치하도록 유지할 수 있게 해주는 단순한 매커니즘을 보유

이 매커니즘을 문서 만료와 재검사라고 부름

### 문서 만료

Cache-Control과 Expires라는 특별한 헤더를 이용해 서버가 각 문서에 유효기간을 붙일 수 있도록 함

Cache-Control: max-age=484200

- 문서의 최대 나이를 정의
- 제공하기 불가하다고 판단하는 경과한 시간 최댓값(초 단위)

Expires

- 절대적인 유효기간을 명시
- 유효기간이 경과되면 그 리소스는 신선하지 않다고 판단

### 서버 재검사

캐시된 문서 만료는 캐시된 리소스가 서버에 존재하는 것과 다르다라는 의미는 아님 => 검사가 필요한 시간이라는 의미

캐시된 문서가 신선한지 검사하는 것이 서버 재검사

**재검사 결과**

- 리소스 변경: 새로운 사본을 가져와 캐싱 후 응답에 전송
- 리소스 변경x: 새 만료일을 포함한 새 헤더들을 가져와서 헤더 갱신

**재검사 방법**

HTTP는 다섯 가지 조건부 요청 헤더를 정의
그 중 2가지가 재검사에 사용 => If-Modified-Since, If-None-Match 헤더

**If-Modifed-Since: 날짜 재검사(가장 흔히 쓰이는 헤더)**

- 주어진 날짜 이후에 변경 => 요청 성공, 새로운 만료 날짜 + 다른 정보가 담긴 헤더를 캐시에 반환
- 주어진 날짜 이후에 변경되지 않음 => 304 Not Modifed 반환
- 서버 응답 헤더의 Last-Modified와 함께 사용

**If-None-Match: 엔터티 태그 재검사**

- 일종의 버전정보인 ETag를 바탕으로 서버에 리소스와 비교하는 방식
- 주어진 날짜로만 검사하는건 명확하지 않은 경우가 많음
- 리소스가 변경됐을 때, 엔터티 태그를 새로운 버전으로 표현
- 캐시된 엔터티 태그가 서버 리소스에 버전과 다르다면 새로운 사본을 받아옴

2가지를 모두 사용하는 것이 효과적

## 캐시 제어

캐시를 제어하는 방법

1. Cache-Control 헤더 첨부
2. Expires 날짜 헤더 첨부
3. 캐시가 스스로 휴리스틱 방법으로 결정하도록 설정

### Cache-Control

Cache-Control: no-store

- 캐시가 응답의 사본을 만드는 것을 금지하는 헤더

Cache-Control: no-cache

- 서버와 재검사를 하지 않고서는 캐시에서 클라이언트로 제공을 금지하는 헤더

### Expires

- Expires: 0 을 보내 항상 만료되어 사용하지 못하도록 함
- 하지만 문법 위반
- 쓰지마라

### 휴리스틱 방법

두 헤더가 없으면 캐시는 경험으로 기한 계산

계산 로직

- 캐시된 문서가 마지막으로 변경된 것이 오래전이라면, 캐시에 오래 보관해도 안전하다고 판단
- 캐시된 문서가 최근에 변경되었다면, 서버와 재검사하기까지 짧은 기간만 캐시 가능하도록 판단

# 🕗 HTTP 완벽 가이드 8장(게이트웨이, 터널, 릴레이) 🕗

게이트웨이

- 서로 다른 프로토콜과 애플리케이션 간의 HTTP 인터페이스

애플리케이션 인터페이스

- 서로 다른 형식의 웹 애플리케이션이 통신하는 데 사용

터널

- HTTP 커넥션을 통해서 HTTP가 아닌 트래픽 전송하는 데 사용

릴레이

- 일종의 단순한 HTTP 프록시, 1번에 1개의 홉에 데이터를 전달

## 게이트웨이

모든 리소스를 하나의 애플리케이션으로만 처리 불가
인터프리터와 같이 리소스를 받기 위한 경로 안내 역할을 하는 게이트웨이 고안

게이트웨이는 HTTP 트래픽을 다른 프로토콜로 자동 변환, HTTP 클라이언트가 다른 프로토콜을 알 필요 업싱 서버에 접속 가능하도록 함

웹 게이트웨이 한 쪽에선 HTTP로 통신, 다른 한 쪽에서는 다른 프로토콜로 통신

클라이언트 측 게이트웨이

- 클라이언트와 외래 프로토콜로 통신, 서버와는 HTTP로 통신

서버 측 게이트웨이

- 클라이언트와 HTTP로 통신, 서버와는 외래 프로토콜로 통신

### 프로토콜 게이트웨이

브라우저에 명시적으로 게이트웨이를 설정하여 자연스럽게 트래픽이 게이트웨이를 거쳐 가게 하거나, 게이트웨이를 대리 서버(대리 프록시)로 설정 가능

HTTP/\*: 서버 측 게이트웨이

하는 일

1. USER과 PASS 명령을 전송해 서버에 로그인
2. 서버에서 적절한 딜게토리로 변경하기 위해 CWD 명령
3. 다운로드 형식 ASCII 설정
4. MDTM으로 문서이 최근 수정 시간 확보
5. PASV로 서버에게 수동형 데이터 검색 선언
6. RETR로 객체 검색
7. 제어 채널에서 반환된 포트로 FTP 서버에 커넥션 연결

HTTP/HTTPS: 서버 측 보안 게이트웨이

- 모든 웹 요청 암호화
- 클라이언트는 HTTP 사용하여 웹 탐색 BUT 게이트웨이는 자동으로 모든 세션 암호화

HTTPS/HTTP : 클라이언트 측 보안 가속 게이트웨이

- 웹 서버 앞에 위치
- 인터셉트 게이트웨이, 대리 프록시 역할
- 보안 트래픽 받아서 복호화, 서버로 보낼 일반 HTTP 요청 생성

### 리소스 게이트웨이

애플리케이션 서버는 HTTP를 통해 클라이언트와 통신, 서버 측에 있는 애플리케이션 프로그램에 연결하는 서버 측 게이트웨이

게이트웨이의 API를 통해 요청을 서버에서 동작하고 있는 애플리케이션으로 전달

최초의 API === CGI(공용 게이트웨이 인터페이스)

- 프로그램 실행, 프로그램 출력 수집, HTTP 응답 회신
- 동적인 HTML, 신용카드 처리, 데이터베이스 질의 등에 사용
- 지금까지도 가장 널리 쓰이는 서버 확장

서버 확장

- 서버 자체의 동작은 바꾸거나, 서버의 처리 능력을 향상하기 위해 제공

### 애플리케이션 인터페이스와 웹 서비스

각 웹 애플리케이션이 서로 통신하는데 사용할 표준과 프로토콜 집합을 개발 === 웹 서비스

SOAP이나 XML을 사용하여 정보 통신
XML - 데이터 객체를 담는 데이터를 생성하고 해석하는 방식
SOAP - HTTP 메시지에 XML 데이터를 담는 방식에 관한 표준

### 터널

HTTP의 또 다른 사용 방식
웹 터널은 HTTP 프로토콜을 지원하지 않는 앱에 HTTP 앱을 사용해 접근하는 방법 제공

웹 터널을 사요앟면 HTTP 커넥션을 통해서 HTTP가 아닌 트래픽 전송 가능

다른 프로토콜을 HTTP 위에 올릴 수 있음

즉, 터널을 사용하는 가장 일반적인 이유는 HTTP 커넥션 안에 HTTP가 아닌 트래픽을 얹기 위함

**CONNECT로 HTTP 터널 커넥션 맺기**

CONNECT - 터널 게이트웨이가 서버에 TCP 커넥션을 맺고 클라이언트와 서버 간에 오는 데이터를 무조건 전달하기로 요청하는 메서드

1. 클라이언트는 게이트웨이에 터널을 연결하려고 CONNECT 요청 전송
2. TCP 커넥션이 생성 완료되면 게이트웨이는 클라이언트에 200 응답 전송
3. 터널 연결
4. 해당 시점부터 커넥션이 끊길 때까지 모든 데이터가 양방향 전달

### 데이터 터널링, 시간, 커넥션 관리

클라이언트는 성능을 높이기 위해 CONNECT 요청을 전송한 뒤, 응답을 받기 전에 터널 데이터를 전송 가능 BUT 게이트웨이가 CONNECT 요청에 이어서 데이터 처리가 가능하다는 전제

문제가 발생했을 때 다시 요청을 보낼 준비도 되어있어야 함

### SSL 터널링

터널을 사용하면 SSL트래픽(암호화 트래픽)을 HTTP 커넥션으로 전송하여 80 포트의 HTTP만을 허용하는 방화벽이라도 통과시킬 수 있음

터널링 기능은 HTTP 메시지에 암호화된 날 데이터를 담고 일반 HTTP 채널을 통해 데이터 전송

### SSL 터널링 VS HTTP/HTTPS 게이트웨이

SSL 터널링을 사용하면 프록시에 SSL 구현 필요 X
SSL 세션은 클라이언트가 생성한 요청과 목적지 서버 간에 생성
프록시 서버는 트랜잭션의 보안에는 관여하지 않고, 암호화된 데이터를 그대로 터널링

### 터널 인증

프록시 인증 기능은 클라이언트가 터널을 사용할 수 있는 권한을 감시하는 용도로 터널에서 사용 가능

외에 HTTP의 다른 기능들은 터널과 함께 적절한 곳에 사용 가능

## 릴레이

HTTP 릴레이는 HTTP 명세를 준수하지 않는 간단한 HTTP 프록시
릴레이는 커넥션을 맺기 위한 통신 뒤에 바이트를 맹목적으로 전달

HTTP는 복잡하기 때문에 간단한 프록시를 구현하는 것이 유용할 때도 있음
하지만 일반적으로 발생하는 악명 높은 문제가 하나 있음

1. 웹 클라이언트는 Connection: Keep-Alive 헤더를 보내 릴레이와 커넥션 유지 요청
2. 릴레이는 이해하지 못해 그대로 서버 전달
3. 서버는 릴레이가 커넥션 유지를 원한다고 판단
4. 릴레이는 받은 유지 요청을 그대로 클라이언트로 전달
5. 클라이언트도 릴레이가 유지를 원한다고 판단
6. 클라이언트는 릴레이에 다음 요청 전송
7. 릴레이는 서버와의 커넥션이 끊기기를 기다림
8. 요청을 계속 무시하면서 기다리기 때문에 아무런 작업도 실행되지 않음

# 🕗 HTTP 완벽 가이드 9장(🌐🤖) 🕗

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 프로그램

로봇은 자동으로 웹 사이트를 탐색하며, 방식에 따라 크롤러, 스파이더, 웜 등 각양각색의 이름으로 불림

크롤러, 크롤링

- 웹 페이지를 가져오고 페이지가 가리키는 모든 웹 페이지를 가져오고 그 페이지들이 가리키는 모든 웹페이들을 가져오는 재귀적 반복으로 웹을 순회하는 로봇
- 웹을 돌아다니면서 모든 문서를 끌어오기 위해 크롤러 사용
- 문서들은 나중에 처리되어 검색 가능한 db로 만들어짐

## 루트 집합

크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합
루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심있는 웹 페이지들의 대부분을 가져올 수 있도록 설정해야 함

### 링크 추출과 상태 링크 정상화

크롤러는 검색한 각 페이지 내부에 URL을 파싱해서 크롤링할 페이지 목록에 추가

### 순환 피하기

로봇들은 순환을 피하기 위해 어디를 방문했는지 기억해야 함

### 루프와 중복

1. 순환은 크롤러를 루프에 빠트려 정지시킬 수 있음
2. 크롤러가 같은 페이지를 반복해서 가져오면 서버 부하
3. 크롤러가 수많은 중복 페이지를 가져옴

### 빵 부스러기

크롤러가 방문한 곳을 관리하기 위해 사용하는 기법

1. 트리&해시 테이블 - 방문한 URL 추적
2. 느슨한 존재 비트맵 - 공간 사용 최소화, 비트 배열과 같은 자료구조 사용
3. 체크포인트 - 로봇이 갑자기 중단될 경우를 대비해, 방문한 URL 목록이 디스크에 저장됐는지 확인
4. 파티셔닝 - URL들의 특정 부분을 할당받아 일 수행

## URL 정규화

1. 포트 번호 명시 X BUT :80을 호스트명에 추가
2. 모든 이스케이핑된 문자를 대응하는 문자로 변환
3. "#" 태그 제거

### 루프와 중복 피하기

1. URL 정규화
2. 너비 우선 크롤링 => 방문할 URL 너비 우선으로 스케쥴링하면 순환 영향 최소화
3. 스로틀링 => 로봇이 일정 시간 동안 가져올 수 있는 페이지 제한
4. URL 크기 제한 => 일정 길이 이상을 가진 URL 크롤링 거부
5. 블랙리스트 => 블랙리스트 목록을 만들어 관리
6. 패턴 발견 => 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 반복된 구성요소로된 URL 크롤링 거부
7. 콘텐츠 지문 => 콘텐츠에 일부로 체크섬 계산
8. 인간 모니터링

## 로봇의 HTTP

로봇 또한 HTTP 프로그램과 다르지 않게 HTTP 명세 규칙을 따라야 함

### 요청 헤더 식별

로봇의 능력, 신원, 출신을 알려주는 헤더를 URL로 전송
기본 식별 헤더 : User-Agent, From, Accpt, Referer

### 가상 호스팅

요청에 Host 헤더를 포함하지 않으면 로봇이 특정 URL에 대해 잘못된 콘텐츠를 읽어올 수 있음
HTTP/1.1이면 Host 헤더 필수

### 조건부 요청

업데이트 상황을 알아보기 위한 조건부 http 요청을 구현함

### 응답

로봇도 여러 종류의 http 응답 다룰 줄 알아야 함

### User-Agent 타겟팅

여러 기능을 지원할 수 있도록 브라우저 종류를 감지하여 콘텐츠 최적화

## 로봇 차단🚫

로봇으로 인해 유발할 수 있는 문제가 많았음
로봇의 동작을 제어하기 위한 기법이 제안, 이 표준은 robots.txt라고 부름

웹 서버 루트에 robots.txt 파일을 선택적으로 제공

어떤 로봇이 서버의 어떤 부분에 접근 가능한지에 대한 정보가 저장됨

표준에 따르는 로봇이라면 리소스에 접근하기 전에 그 사이트의 robots.txt를 요청 => 권한 확인을 위해 해당 파일 검사

### 웹 사이트와 🤖s.txt

로봇은 HTTP GET 메서드를 통해 서버에서 robots.txt 요청
파일이 존재하면 text/plain 본문으로 반환

많은 웹 사이트가 robots.txt가 없지만, 로봇은 이를 모르기 때문에 해당 요청에 대한 응답에 따라 다르게 동작

HTTP 상태 코드 2XX : 그 응답의 콘텐츠를 파싱하여 차단 규칙을 얻어 따른다
HTTP 상태 코드 404 : 차단 규칙이 없다고 가정하고 robots.txt의 제약 없이 접근한다
HTTP 상태 코드 401, 403 : 접근이 완전히 제한되어있다고 가정
HTTP 상태 코드 503 : 그 사이트의 리소스를 검색하는 것은 뒤로 미루어야 한다
HTTP 상태 코드 3XX : 로봇은 리소스가 발견될 때까지 리다이렉트를 따라가야 한다

### 파싱 규칙

1. 로봇은 자신이 이해하지 못하는 필드는 무시
2. 하위 호환성을 위해 한 줄을 여러 줄로 나누어 적는 것은 비허용
3. 주석은 허용
4. 버전 0.0은 ALLOW 줄 지원 X, 허용되는 URI도 탐색하지 않는 경우 발생

## 스푸핑

웹 마스터는 본인의 웹 사이트가 상단에 노출되도록 해야 함
검색 결과에 더 높은 순위를 차지하고자 수많은 키워드를 나열한 가짜 페이지를 만들거나, 검색 엔진 알고리즘을 속일 수 있는 특정 단어가 포함된 페이지를 생성하는 게이트웨이 애플리케이션을 만들어 사용
